# <h1 align=center> Awesome Multimodal Learning üé∂üìú</h1>



<div align=center>

<p>
 
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)<br>
![GitHub stars](https://img.shields.io/github/stars/csyfjiang/awesome-multimodal-learning.svg?color=red&style=for-the-badge) ![GitHub forks](https://img.shields.io/github/forks/csyfjiang/awesome-multimodal-learning.svg?color=yellow&style=for-the-badge) ![GitHub activity](https://img.shields.io/github/last-commit/csyfjiang/awesome-multimodal-learning?style=for-the-badge) 
<a href="#">
      <img src="https://api.visitorbadge.io/api/VisitorHit?user=csyfjiang&repo=awesome-multimodal-learning&countColor=%237B1E7A"/>
</a>
[![Star History Chart](https://api.star-history.com/svg?repos=csyfjiang/awesome-multimodal-learning&type=Date)](https://star-history.com/#csyfjiang/awesome-multimodal-learning&Date)
</p>

This is a comprehensive list of multimodal representation learning, pre-training(semi-, self-, supervised, etc.), and some basic concepts for research and implementation.
 
</div>

## <span id="head1"> *1.Collection Documentation* </span>

>üêå Markdown Format:
>
> * (Conference/Journal Year) **Title**, First Author et al. [[Paper](URL)] [[Code](URL)] [[Project](URL)] <br/>
> * (Conference/Journal Year) [üí¨Topic] **Title**, First Author et al. [[Paper](URL)] [[Code](URL)] [[Project](URL)]
>     * (Optional) ```üå±``` or ```üìå ```
>     * (Optional) üöÄ or üëë or üìö

* ```üå±: Novel idea```
* ```üìå: The first...```
* üöÄ: State-of-the-Art
* üëë: Novel dataset/model
* üìö: Downstream Tasks
* ‚öôÔ∏è: Application

## <span id="head2"> *2. Topic Order* </span>
### <span id="head21"> *Survey Paper* </span>
* <span id="head-survey"> **[DL Survey credits from Zhou's collections](https://github.com/Yutong-Zhou-cv/Awesome-Survey-Papers)**  </span>

#### <span id="head211"> *Pre-training | training strategy* </span>

* (TPAMI 2023, arXiv 2024) **Self-Supervised Multimodal Learning: A Survey**, First Author et al. [[Paper](URL)] [[Code](URL)] [[Project](URL)] <br/>
* (Conference/Journal Year) **Title**, First Author et al. [[Paper](URL)] [[Code](URL)] [[Project](URL)] <br/>
* (Conference/Journal Year) **Title**, First Author et al. [[Paper](URL)] [[Code](URL)] [[Project](URL)] <br/>

#### <span id="head212"> *Not Classified* </span>

* (TPAMI 2023) **Multimodal Image Synthesis and Editing: A Survey and Taxonomy**, Fangneng Zhan et al. [[v1](https://arxiv.org/abs/2112.13592v1)](2021.12.27) ... [[v5](https://arxiv.org/abs/2112.13592v5)](2023.08.05)
* (TPAMI 2023) [üí¨Transformer] **Multimodal Learning with Transformers: A Survey**, Peng Xu et al. [[v1](https://arxiv.org/abs/2206.06488)](2022.06.13) [[v2](https://ieeexplore.ieee.org/abstract/document/10123038)](2023.05.11)
* (Multimedia Tools and Applications) **A comprehensive survey on generative adversarial networks used for synthesizing multimedia content**, Lalit Kumar & Dushyant Kumar Singh [[v1](https://link.springer.com/article/10.1007/s11042-023-15138-x#Sec47)](2023.03.30)
* ‚≠ê‚≠ê(arXiv 2023) **Multimodal Deep Learning**, Cem Akkus et al. [[v1](https://arxiv.org/abs/2301.04856)](2023.01.12)
* ‚≠ê(arXiv 2022) [üí¨Knowledge Enhanced] **A survey on knowledge-enhanced multimodal learning**, Maria Lymperaiou et al. [[v1](https://arxiv.org/abs/2211.12328)](2022.11.19)
* ‚≠ê‚≠ê(arXiv 2022) **Vision-Language Pre-training: Basics, Recent Advances, and Future Trends**, Zhe Gan et al. [[v1](https://arxiv.org/abs/2210.09263)](2022.10.17)
* ‚≠ê(arXiv 2022) **Vision+X: A Survey on Multimodal Learning in the Light of Data**, Ye Zhu et al. [[v1](https://arxiv.org/abs/2210.02884)](2022.10.05)
* (arXiv 2022) **Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions**, Paul Pu Liang et al. [[v1](https://arxiv.org/abs/2209.03430)](2022.09.07)
* (arXiv 2022) [üí¨Cardiac Image Computing] **Multi-Modality Cardiac Image Computing: A Survey**, Lei Li et al. [[v1](https://arxiv.org/pdf/2208.12881.pdf)](2022.08.26)
* (arXiv 2022) [üí¨Vision and language Pre-training (VLP)] **Vision-and-Language Pretraining**, Thong Nguyen et al. [[v1](https://arxiv.org/abs/2207.01772)](2022.07.05)
* (arXiv 2022) [üí¨Video Saliency Detection] **A Comprehensive Survey on Video Saliency Detection with Auditory Information: the Audio-visual Consistency Perceptual is the Key!**, Chenglizhao Chen et al. [[v1](https://arxiv.org/abs/2206.13390)](2022.06.20)
* (arXiv 2022) [üí¨Vision and language Pre-training (VLP)] **Vision-and-Language Pretrained Models: A Survey**, Siqu Long et al. [[v1](https://arxiv.org/abs/2204.07356v1)](2022.04.15)...[[v5](https://arxiv.org/abs/2204.07356v5)](2022.05.03) 
* (arXiv 2022) [üí¨Vision and language Pre-training (VLP)] **VLP: A Survey on Vision-Language Pre-training**, Feilong Chen et al. [[v1](https://arxiv.org/abs/2202.09061v1)](2022.02.18) [[v2](https://arxiv.org/abs/2202.09061v2)](2022.02.21) 
* (arXiv 2022) [üí¨Vision and language Pre-training (VLP)] **A Survey of Vision-Language Pre-Trained Models**, Yifan Du et al. [[v1](https://arxiv.org/abs/2202.10936)](2022.02.18) 
* (arXiv 2022) [üí¨Multi-Modal Knowledge Graph] **Multi-Modal Knowledge Graph Construction and Application: A Survey**, Xiangru Zhu et al. [[v1](https://arxiv.org/pdf/2202.05786.pdf)](2022.02.11) 
* (arXiv 2022) [üí¨Auto Driving] **Multi-modal Sensor Fusion for Auto Driving Perception: A Survey**, Keli Huang et al. [[v1](https://arxiv.org/abs/2202.02703v1)](2022.02.06) [[v2](https://arxiv.org/abs/2202.02703)](2022.02.27) 
* (arXiv 2021) **A Survey on Multi-modal Summarization**, Anubhav Jangra et al. [[v1](https://arxiv.org/pdf/2109.05199.pdf)](2021.09.11) 
* (Information Fusion 2021) [üí¨Vision and language] **Multimodal research in vision and language: A review of current and emerging trends**, ShagunUppal et al. [[v1](https://www.sciencedirect.com/science/article/pii/S1566253521001512)](2021.08.01) 

## <span id="head3"> *3. Chronological Order* </span>
## <span id="head4"> *4.Courses* </span>
## <span id="head4"> *5.Foundamental* </span>
## <span id="head4"> *6.Acknowledgement* </span>

> This style credits from  [Yutong Zhou](https://github.com/Yutong-Zhou-cv/Awesome-Multimodality)
